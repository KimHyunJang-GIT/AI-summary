from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional
import time
import traceback # traceback ëª¨ë“ˆ ì„í¬íŠ¸
import sys # sys ëª¨ë“ˆ ì„í¬íŠ¸

import google.generativeai as genai # Gemini API ì„í¬íŠ¸
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM # Hugging Face pipeline, AutoTokenizer, AutoModelForCausalLM ì„í¬íŠ¸
from src.config import DEFAULT_TOP_K, DEFAULT_SIMILARITY_THRESHOLD, GEMINI_API_KEY, USE_LOCAL_LLM, LOCAL_LLM_MODEL_NAME, LOCAL_LLM_MAX_NEW_TOKENS # GEMINI_API_KEY ë° ë¡œì»¬ LLM ì„¤ì • ì„í¬íŠ¸
from src.core.retrieval import Retriever
from src.core.utils import StartupSpinner as Spinner # ê²½ë¡œ ìˆ˜ì •

@dataclass
class ChatTurn:
    role: str
    text: str
    hits: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class LNPChat:
    corpus_path: Path
    cache_dir: Path
    topk: int = DEFAULT_TOP_K
    similarity_threshold: float = DEFAULT_SIMILARITY_THRESHOLD
    translation_cache: Optional[Dict[str, str]] = None # ë²ˆì—­ ìºì‹œ ì£¼ì… ì¸í„°í˜ì´ìŠ¤

    retr: Optional[Retriever] = field(init=False, default=None)
    history: List[ChatTurn] = field(init=False, default_factory=list)
    ready_done: bool = field(init=False, default=False)
    gemini_model: Any = field(init=False, default=None) # Gemini ëª¨ë¸ í•„ë“œ ì¶”ê°€
    local_llm_pipeline: Any = field(init=False, default=None) # ë¡œì»¬ LLM íŒŒì´í”„ë¼ì¸ í•„ë“œ ì¶”ê°€
    local_llm_max_input_length: int = field(init=False, default=0) # ë¡œì»¬ LLM ìµœëŒ€ ì…ë ¥ ê¸¸ì´ í•„ë“œ ì¶”ê°€
    local_llm_tokenizer: Any = field(init=False, default=None) # ë¡œì»¬ LLM í† í¬ë‚˜ì´ì € í•„ë“œ ì¶”ê°€

    def __post_init__(self):
        # Gemini ëª¨ë¸ ì´ˆê¸°í™”
        if not USE_LOCAL_LLM:
            if GEMINI_API_KEY and GEMINI_API_KEY != "YOUR_GEMINI_API_KEY":
                try:
                    genai.configure(api_key=GEMINI_API_KEY)
                    self.gemini_model = genai.GenerativeModel('gemini-pro')
                    print("âœ… Gemini model initialized successfully.")
                except Exception as e:
                    print(f"âŒ Failed to initialize Gemini model: {e}", file=sys.stderr)
                    traceback.print_exc(file=sys.stderr)
                    sys.stderr.flush()
            else:
                print("âš ï¸ GEMINI_API_KEY is not set or is default. Conversational features will be limited.", file=sys.stderr)
        else: # ë¡œì»¬ LLM ì‚¬ìš© ì‹œ
            try:
                print(f"ğŸ§  Loading local LLM: {LOCAL_LLM_MODEL_NAME}...")
                # Explicitly load tokenizer and model to get max_position_embeddings
                self.local_llm_tokenizer = AutoTokenizer.from_pretrained(LOCAL_LLM_MODEL_NAME)
                model = AutoModelForCausalLM.from_pretrained(LOCAL_LLM_MODEL_NAME)

                self.local_llm_max_input_length = model.config.max_position_embeddings
                self.local_llm_pipeline = pipeline(
                    "text-generation", 
                    model=model,
                    tokenizer=self.local_llm_tokenizer,
                    # max_lengthì™€ truncationì€ pipeline í˜¸ì¶œ ì‹œì ì— ì „ë‹¬í•˜ì—¬ ë™ì ìœ¼ë¡œ ì œì–´
                    # device=0 if torch.cuda.is_available() else -1 # GPU ì‚¬ìš© ì‹œ ì£¼ì„ í•´ì œ ë° torch ì„í¬íŠ¸ í•„ìš”
                )
                # Set pad_token_id if not already set (common for GPT-like models)
                if self.local_llm_tokenizer.pad_token is None:
                    self.local_llm_tokenizer.pad_token = self.local_llm_tokenizer.eos_token

                print(f"âœ… Local LLM initialized successfully. Max input length: {self.local_llm_max_input_length}")
            except ImportError:
                print("âŒ 'transformers' library not found. Please install it: pip install transformers", file=sys.stderr)
            except Exception as e:
                print(f"âŒ Failed to initialize local LLM: {e}", file=sys.stderr)
                traceback.print_exc(file=sys.stderr)
                sys.stderr.flush()

    def ready(self, rebuild: bool = False):
        spin = Spinner(prefix="ì—”ì§„ ì´ˆê¸°í™”")
        spin.start()
        try:
            self.retr = Retriever(corpus_path=self.corpus_path, cache_dir=self.cache_dir)
            self.retr.ready(rebuild=rebuild)
            self.ready_done = True
        finally:
            spin.stop()
        print("âœ… LNP Chat ì¤€ë¹„ ì™„ë£Œ")

    def _is_conversational_query(self, query: str) -> bool:
        """
        Determines if a query is conversational rather than a search query.
        This is a simple heuristic and can be improved with NLU models.
        """
        query_lower = query.lower().strip()
        conversational_keywords = [
            "ì•ˆë…•", "ë°˜ê°€ì›Œ", "ì•ˆë…•í•˜ì„¸ìš”", "ì˜ ì§€ë‚´", "ë­í•´", "ê³ ë§ˆì›Œ", "ê°ì‚¬í•©ë‹ˆë‹¤",
            "ëˆ„êµ¬ì„¸ìš”", "ë¬´ì—‡ì„ í•  ìˆ˜ ìˆë‚˜ìš”", "ë„ì™€ì¤˜", "ì•Œê² ìŠµë‹ˆë‹¤", "ë„¤", "ì•„ë‹ˆì˜¤",
            "ê³ ë§ˆì›Œìš”", "ì²œë§Œì—ìš”", "ìˆ˜ê³ í–ˆì–´ìš”", "ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤", "ì¢‹ì•„ìš”", "ì‹«ì–´ìš”",
            "ì‘", "ì•„ë‹ˆ", "ê·¸ë˜", "ì•„ë‹ˆì•¼", "ë§ì•„", "í‹€ë ¤", "ì™œ", "ì–´ë–»ê²Œ", "ì–¸ì œ", "ì–´ë””ì„œ", "ëˆ„ê°€", "ë¬´ì—‡ì„"
        ]
        
        # Check if the query is a greeting or a very general conversational phrase
        if any(keyword in query_lower for keyword in conversational_keywords):
            return True
        
        # Check if the query is very short and not clearly a search term
        if len(query_lower.split()) <= 3 and not any(word in query_lower for word in ["ë¬¸ì„œ", "íŒŒì¼", "ê²€ìƒ‰", "ì°¾ì•„", "ìš”ì•½", "í‘œ", "ì •ë¦¬"]):
            return True
            
        return False

    def _get_llm_response(self, query: str, history_context: Optional[List[Dict[str, Any]]] = None) -> str:
        """
        Integrates with Google Gemini API or a local Hugging Face LLM for conversational responses.
        """
        if USE_LOCAL_LLM:
            if not self.local_llm_pipeline or not self.local_llm_tokenizer:
                return "ë¡œì»¬ LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ëª¨ë¸ ë¡œë”©ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            try:
                # Convert history_context to a more structured prompt for local LLM
                prompt_parts = []
                
                # System message to guide the LLM
                prompt_parts.append("ë‹¹ì‹ ì€ InfoPilotì´ë¼ëŠ” ì´ë¦„ì˜ ì¹œì ˆí•˜ê³  ìœ ëŠ¥í•œ í•œêµ­ì–´ ì±—ë´‡ì…ë‹ˆë‹¤. ì‚¬ìš©ìì—ê²Œ ìì—°ìŠ¤ëŸ½ê³  ê³µì†í•˜ê²Œ ì‘ë‹µí•´ì£¼ì„¸ìš”. ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ í•´ì£¼ì„¸ìš”.")
                prompt_parts.append("\n--- ëŒ€í™” ì‹œì‘ ---") # ëŒ€í™” ì˜ˆì‹œ ì‹œì‘ì„ ëª…í™•íˆ
                
                # Add few-shot examples
                prompt_parts.append("ì‚¬ìš©ì: ì•ˆë…• ë°˜ê°€ì›Œ")
                prompt_parts.append("InfoPilot: ì•ˆë…•í•˜ì„¸ìš”! ë§Œë‚˜ì„œ ë°˜ê°‘ìŠµë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
                prompt_parts.append("ì‚¬ìš©ì: ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?")
                prompt_parts.append("InfoPilot: ì €ëŠ” ë‚ ì”¨ ì •ë³´ë¥¼ ì•Œ ìˆ˜ ì—†ì§€ë§Œ, ì˜¤ëŠ˜ í•˜ë£¨ ì¦ê²ê²Œ ë³´ë‚´ì‹œê¸¸ ë°”ëë‹ˆë‹¤!")
                
                # Add history context
                if history_context:
                    # Only include the last few turns to keep the prompt short for small models
                    for turn in history_context[-2:]: # Consider only last 2 turns for brevity
                        role_prefix = "ì‚¬ìš©ì: " if turn.get("role") == "user" else "InfoPilot: "
                        prompt_parts.append(f"{role_prefix}{turn.get('text', '')}")
                
                # Add current query
                prompt_parts.append(f"ì‚¬ìš©ì: {query}")
                prompt_parts.append("InfoPilot: ") # LLMì´ ë‹µë³€ì„ ì‹œì‘í•˜ë„ë¡ ìœ ë„
                
                full_prompt = "\n".join(prompt_parts)

                # Explicitly tokenize the prompt and prepare for generation
                inputs = self.local_llm_tokenizer(
                    full_prompt,
                    return_tensors="pt",
                    max_length=self.local_llm_max_input_length, # ëª¨ë¸ì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ ì‚¬ìš©
                    truncation=True,
                    padding=True # íŒ¨ë”© ë‹¤ì‹œ ì¶”ê°€
                )

                # Move inputs to the same device as the model (if using GPU, requires torch import)
                # if torch.cuda.is_available():
                #     inputs = {k: v.to(self.local_llm_pipeline.device) for k, v in inputs.items()}
                
                # Generate response using the underlying model's generate method
                output_sequences = self.local_llm_pipeline.model.generate(
                    input_ids=inputs["input_ids"],
                    attention_mask=inputs["attention_mask"],
                    max_new_tokens=LOCAL_LLM_MAX_NEW_TOKENS,
                    num_return_sequences=1,
                    pad_token_id=self.local_llm_tokenizer.pad_token_id, # Explicitly set pad_token_id
                    eos_token_id=self.local_llm_tokenizer.eos_token_id,
                    max_length=self.local_llm_max_input_length # ìƒì„±ë  ì „ì²´ ì‹œí€€ìŠ¤ ê¸¸ì´ ì œí•œ
                )

                # Decode the generated text
                generated_text = self.local_llm_tokenizer.decode(
                    output_sequences[0], skip_special_tokens=True
                )

                # Extract only the newly generated part
                # Decode the input_ids to get the exact input text that was fed to the model
                input_text_decoded = self.local_llm_tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True)
                
                answer = generated_text[len(input_text_decoded):].strip()

                # Clean up any remaining prompt parts if the model repeats them
                if answer.startswith("InfoPilot:"):
                    answer = answer[len("InfoPilot:"):].strip()

                # Remove any trailing "ì‚¬ìš©ì:" or "InfoPilot:" if the model generates incomplete turns
                if "ì‚¬ìš©ì:" in answer:
                    answer = answer.split("ì‚¬ìš©ì:")[0].strip()
                if "InfoPilot:" in answer:
                    answer = answer.split("InfoPilot:")[0].strip()

                return answer
            except Exception as e:
                print(f"âŒ Error calling local LLM: {e}", file=sys.stderr)
                traceback.print_exc(file=sys.stderr)
                sys.stderr.flush()
                return "ë¡œì»¬ LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        else: # Gemini ì‚¬ìš©
            if not self.gemini_model:
                return "Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. API í‚¤ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

            try:
                # Convert history_context to Gemini's format
                gemini_history = []
                if history_context:
                    for turn in history_context:
                        role = "user" if turn.get("role") == "user" else "model" # Gemini expects 'model' for assistant
                        gemini_history.append({"role": role, "parts": [turn.get("text", "")]})

                chat = self.gemini_model.start_chat(history=gemini_history)
                response = chat.send_message(query)
                return response.text
            except Exception as e:
                print(f"âŒ Error calling Gemini API: {e}", file=sys.stderr)
                traceback.print_exc(file=sys.stderr)
                sys.stderr.flush()
                return "Gemini API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

    def ask(self, query: str, topk: Optional[int] = None, filters: Optional[Dict[str, Any]] = None, use_bm25: bool = False, use_reranker: bool = False, history_context: Optional[List[Dict[str, Any]]] = None) -> Dict[str, Any]: # history_context íƒ€ì… íŒíŠ¸ ìˆ˜ì •
        print(f"DEBUG: LNPChat.ask received query: '{query}'") # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        if not self.ready_done: self.ready(rebuild=False)
        
        # ë¹ˆ ì¿¼ë¦¬ ì²˜ë¦¬
        if not query.strip() and not history_context: # history_contextê°€ ì—†ìœ¼ë©´ ë¹ˆ ì¿¼ë¦¬ ì²˜ë¦¬
            return {"answer": "ì§ˆì˜ ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.", "hits": [], "suggestions": self._suggest_followups(query, [])}

        # 1. ì˜ë„ ë¶„ë¥˜: ëŒ€í™”í˜• ì¿¼ë¦¬ì¸ì§€ í™•ì¸
        if self._is_conversational_query(query):
            llm_answer = self._get_llm_response(query, history_context)
            return {"answer": llm_answer, "hits": [], "suggestions": []} # ëŒ€í™”í˜• ì‘ë‹µì€ hitsì™€ suggestions ì—†ìŒ

        k = topk or self.topk
        spin = Spinner(prefix="ê²€ìƒ‰ ì¤‘")
        spin.start()
        t0 = time.time()

        # history_contextë¥¼ í™œìš©í•˜ì—¬ ì¿¼ë¦¬ ë³´ê°•
        enriched_query = query
        if history_context:
            context_texts = [turn["text"] for turn in history_context if "text" in turn]
            if context_texts:
                enriched_query = " ".join(context_texts) + " " + query
            print(f"DEBUG: Enriched query with history: '{enriched_query}'")

        try:
            # Retriever.searchì— enriched_query ì „ë‹¬
            candidate_hits = self.retr.search(enriched_query, top_k=max(k * 2, 20), filters=filters, use_bm25=use_bm25, use_reranker=use_reranker, history_context=history_context)
        except Exception as e:
            print(f"âŒ LNPChat.ask: Error during self.retr.search: {e}", file=sys.stderr)
            traceback.print_exc(file=sys.stderr)
            sys.stderr.flush() # ì˜¤ë¥˜ ì¦‰ì‹œ ì¶œë ¥
            raise # ì˜¤ë¥˜ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ FastAPIì—ì„œ 500 ì—ëŸ¬ë¥¼ ë°˜í™˜í•˜ë„ë¡ í•¨
        finally:
            spin.stop()
        dt = time.time() - t0
        filtered_hits = [h for h in candidate_hits if h['similarity'] >= self.similarity_threshold]
        final_hits = filtered_hits[:k]
        
        # LNPChat ë‚´ë¶€ historyëŠ” FastAPIì˜ chat_endpointì—ì„œ ê´€ë¦¬í•˜ë„ë¡ ë³€ê²½
        # self.history.append(ChatTurn(role="user", text=query))
        # self.history.append(ChatTurn(role="assistant", text="", hits=final_hits))
        
        if not final_hits:
            answer_lines = [f"â€˜{query}â€™ì™€ ê´€ë ¨ëœ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]
        else:
            answer_lines = [f"â€˜{query}â€™ì™€(ê³¼) ì˜ë¯¸ìƒ ìœ ì‚¬í•œ ë¬¸ì„œ Top {len(final_hits)} (ê²€ìƒ‰ {dt:.2f}s):"]
            for i, h in enumerate(final_hits, 1):
                sim = f"{h['similarity']:.3f}"
                answer_lines.append(f"{i}. {h['path']}  (ìœ ì‚¬ë„: {sim})")
                if h.get("summary"):
                    answer_lines.append(f"   ìš”ì•½: {h['summary']}")
        return {"answer": "\n".join(answer_lines), "hits": final_hits, "suggestions": self._suggest_followups(query, final_hits)}

    def summarize_document(self, target_info: Dict[str, Any]) -> str:
        """
        Summarizes the content of a document given its path.
        target_info is expected to contain 'path'.
        """
        doc_path = target_info.get('path')
        if not doc_path:
            return "ìš”ì•½í•  ë¬¸ì„œ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        if not self.retr: # Retrieverê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ë‹¤ë©´
            self.ready(rebuild=False)
            if not self.retr:
                return "ë¬¸ì„œ ìš”ì•½ ì—”ì§„ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        full_content = self.retr.get_document_content(doc_path)
        if not full_content:
            return f"'{doc_path}' ê²½ë¡œì˜ ë¬¸ì„œ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ê°„ë‹¨í•œ ìš”ì§: ì²˜ìŒ 300ì ë˜ëŠ” ì²« 3ë¬¸ì¥
        summary_length = 300
        if len(full_content) > summary_length:
            # ì²« 300ì ì´í›„ ê°€ì¥ ê°€ê¹Œìš´ ë§ˆì¹¨í‘œë¥¼ ì°¾ì•„ ìë¥´ê¸°
            end_index = full_content.find('.', summary_length)
            if end_index == -1: # ë§ˆì¹¨í‘œê°€ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ìë¥´ê¸°
                end_index = summary_length
            summary = full_content[:end_index+1].strip()
            if len(full_content) > end_index + 1: # ì›ë³¸ í…ìŠ¤íŠ¸ê°€ ë” ê¸¸ë©´ ... ì¶”ê°€
                summary += "..."
        else:
            summary = full_content.strip()

        return f"'{doc_path}' ë¬¸ì„œ ìš”ì•½:\n\n{summary}"

    def find_similar_documents(self, target_info: Dict[str, Any], topk: Optional[int] = None) -> List[Dict[str, Any]]:
        """
        Finds similar documents based on a given document path or query text.
        target_info is expected to contain 'path' or 'query_text'.
        """
        base_query_text = target_info.get('query_text')
        doc_path = target_info.get('path')
        
        search_query = ""
        if doc_path:
            if not self.retr:
                self.ready(rebuild=False)
                if not self.retr:
                    return [] # Retriever not ready
            doc_content = self.retr.get_document_content(doc_path)
            if doc_content:
                search_query = doc_content
            else:
                return [] # Document content not found
        elif base_query_text:
            search_query = base_query_text
        else:
            return [] # No valid base for search

        if not search_query.strip():
            return [] # Empty search query

        k = topk or self.topk
        
        # Perform search using the determined query
        similar_hits = self.retr.search(
            search_query,
            top_k=k + 1, # Retrieve one more to potentially exclude the original document
            use_bm25=True, # ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì‹œ BM25ì™€ ì¬ë­í‚¹ì„ í™œìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 
            use_reranker=True
        )

        final_similar_hits = []
        if doc_path: # If search was based on a document, exclude it from results
            for hit in similar_hits:
                if hit.get('path') != doc_path:
                    final_similar_hits.append(hit)
        else:
            final_similar_hits = similar_hits

        return final_similar_hits[:k]

    def format_hits_as_table(self, hits: List[Dict[str, Any]]) -> str:
        """
        Formats a list of hits into a simple table string.
        """
        if not hits:
            return "í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•  ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

        table_lines = ["| ìˆœë²ˆ | íŒŒì¼ ê²½ë¡œ | ìœ ì‚¬ë„ | ìš”ì•½ |", "|---|---|---|---|"]
        for i, hit in enumerate(hits, 1):
            path = hit.get('path', 'N/A')
            similarity = f"{hit.get('similarity', 0.0):.3f}"
            summary = hit.get('summary', 'ìš”ì•½ ì—†ìŒ')
            table_lines.append(f"| {i} | {path} | {similarity} | {summary} |")
        return "\n".join(table_lines)

    def _suggest_followups(self, query: str, hits: List[Dict[str, Any]]) -> List[str]:
        base = ["ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•´ì¤˜", "ìœ„ ë¬¸ì„œë“¤ê³¼ ë¹„ìŠ·í•œ ë‹¤ë¥¸ ë¬¸ì„œë¥¼ ë” ì°¾ì•„ì¤˜", "ê²°ê³¼ë¥¼ í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•´ì¤˜"] if hits else ["ë‹¤ë¥¸ í‘œí˜„ìœ¼ë¡œ ê°™ì€ ì˜ë¯¸ì˜ ì§ˆì˜ë¥¼ ì‹œë„", "ë¬¸ì„œ ìœ í˜•(ì—‘ì…€/í•œê¸€/PDF ë“±)ì„ ì§€ì •í•´ì„œ ê²€ìƒ‰"]
        seen, out = set(), []
        for s in base:
            if s not in seen: out.append(s); seen.add(s)
        return out[:3]
